# Словари и множества

Множество отличается от словаря тем, что он не имеет значение, а только ключ. Ключ должен быть хэшируемым, то есть реализующий методы `__hash__` и либо `__eq__`, либо `__cmp__`. По умолчанию пользовательские классы тоже хэшируемы.

Эти структуры данных имеют сложность O(1) для вставки, удаления и получения элемента. Однако скорость работы зависит непосредственно от реализации хэширующей функции.

### Как это работает?

Словари и множества используют хэш-таблицы для достижения сложности операций O(1). Такой результат достигается использованием хэш-функции, которая превращает ключ в индекс для списка.

Место вставки зависит от двух параметров: хэш ключа и как этот ключ соотносится с другими ключами.

Если мы выделили 8 блоков памяти, а наше хэш-значение равно 28975, то индекс нашего bucket равен `28975 & 0b111 = 7`. Маска равна `bin(N - 1)`, где N - кол-во выделенных блоков памяти.

Если bucket пуст, то вставляем ключ и значение сюда. Ключ хранится для уверенности, что мы достанем нужное значение. Если bucket занят и ключ совпадает, возвращаем значение. Иначе нужно найти другое место.

Для дополнительной оптимизации Python сначала добавляет ключ-значение в обычный массив, а затем заносит только индекс в хэш-таблицу. Это позволяет сократить расходы памяти на 30-95%.

Чтобы найти новый индекс, мы вычисляем линейную функцию с помощью зондирования. Пример модифицированного линейного зондирования:

```python
def index_sequence(key, mask=0b111, PERTURB_SHIFT=5):
    perturb = hash(key)
    i = perturb & mask
    yield i
    while True:
        perturb >>= PERTURB_SHIFT
        i = (i * 5 + perturb + 1) & mask
        yield i
```

Так будет меняться индекс:

```
First 10 samples for hash <ForceHash 0b00000111>: [7, 4, 5, 2, 3, 0, 1, 6, 7, 4]
First 10 samples for hash <ForceHash 0b11100111>: [7, 3, 0, 1, 6, 7, 4, 5, 2, 3]
First 10 samples for hash <ForceHash 0b01110111>: [7, 7, 4, 5, 2, 3, 0, 1, 6, 7]
First 10 samples for hash <ForceHash 0b01110001>: [1, 1, 6, 7, 4, 5, 2, 3, 0, 1]
First 10 samples for hash <ForceHash 0b01110000>: [0, 4, 5, 2, 3, 0, 1, 6, 7, 4]
```

Однако если N последних битов будут одинаковыми, то будет не просто коллизия, но еще и совпадение всей последовательности зондирования. Проблема решается рассмотрением большего количества последних битов.

В следующем примере в качестве хэш-функции используется порядковый номер первого символа:

```python
class City(str):
    def __hash__(self):
        return ord(self[0])


if __name__ == "__main__":
    # We create a dictionary where we assign arbitrary values to cities
    data = {
        City("Rome"): "Italy",
        City("San Francisco"): "USA",
        City("New York"): "USA",
        City("Barcelona"): "Spain",
    }
```

У Barcelona и Rome возникнет коллизия. Для словаря с 4 элементами мы имеем маску `0b111`, поэтому они будут пытаться использовать одинаковый индекс 2:

```
hash('Barcelona')   = ord('B') & 0b111 = 66 & 0b111 = 2
hash('Rome')        = ord('R') & 0b111 = 82 & 0b111 = 2
```

Когда количество элементов достигает более 2/3 от объема, он расширяется. Сохранение одной трети как раз таки нужно выделить для случаев коллизии. При расширении все элементы старой таблицы вставляются в новую. Звучит довольно затратно, особенно для больших таблиц. Тем не менее, амортизированная стоимость добавления элемента будет равна O(1).

Например, если изначально объем равен 8, то при добавлении 6-го элемента объем увеличится до 3 * 6 = 18. Далее при добавлении 13-го элемента - до 3 * 13 = 39 и т.д.. Последовательность выглядит так:

```
8, 18, 39, 81, 165, 669, 1_341, 2_685, 5_373, 10_749, 21_501, 43_005, ...
```

При удалении уменьшения таблицы не происходит. На месте удаляемого элемента будет пустая ячейка.

### Хэш-функции и энтропия

Все объекты в Python могут хэшироваться, так как object имеет встроенные методы `__hash__` и `__cmp__`. Для числовых типов хэш основывается на их битовом представлении. Для кортежей и строк - на их элементах. Списки не поддерживают хэширование, потому что их значение может поменяться - реаллокация поменяет ссылку.

Если пользовательский класс явным образом не определяет эти методы, то он все равно хэшируется:

```python
class Point(object):
    def __init__(self, x, y):
        self.x, self.y = x, y

p1 = Point(1, 1)
p2 = Point(1, 1)
points = {p1, p2}
print(points) # set([<__main__.Point at 0x1099bfc90>, <__main__.Point at 0x1099bfbd0>])
print(Point(1, 1) in points) # False
```

От хэш-функции зависит скорость работы. В худшем случае, когда все ключи друг с другом вызывают коллизии, сложность получения элемента составит O(n) из-за зондирования.

**Энтропия** показывает насколько хорошо хэш-функция распределяет по bucket'ам. Формула такая:

```
S = -sum(p(i) * log(p(i))
```

где `p(i)` - вероятность того, что хэш-функция выдаст хэш `i`. Энтропия максимальна, когда каждый хэш имеет одинаковую вероятность. Такая хэш-функция называется _идеальной_ и гарантирует минимальное количество коллизий.

Хэш-функция для целых чисел будет _идеальной_ при условии, что словарь будет бесконечным, т.к. хэш целого числа равен самому числу.

Пример ниже показывает, насколько сильно может отличаться время работы:

```python
import string
import timeit

class BadHash(str):
    def __hash__(self):
        return 42

class GoodHash(str):
    def __hash__(self):
        return ord(self[1]) + 26 * ord(self[0]) - 2619

baddict = set()
gooddict = set()
for i in string.ascii_lowercase:
    for j in string.ascii_lowercase:
        key = i + j
        baddict.add(BadHash(key))
        gooddict.add(GoodHash(key))

badtime = timeit.repeat(
    "key in baddict",
    setup="from __main__ import baddict, BadHash; key = BadHash('zz')",
    repeat=3,
    number=100_000,
)
goodtime = timeit.repeat(
    "key in gooddict",
    setup="from __main__ import gooddict, GoodHash; key = GoodHash('zz')",
    repeat=3,
    number=100_000,
)

print(f"Min lookup time for baddict: {min(badtime)}")
print(f"Min lookup time for gooddict: {min(goodtime)}")
```

```
Min lookup time for baddict: 1.4021705739996833
Min lookup time for gooddict: 0.022241179999582528
```

### Пространство имен

Управление пространством имен в Python основано на словарях. Обращение к переменной, функции или модуля вызывает длинную цепочку поиска объекта в Python.

Сначала идет поиск в `locals()`. Если не найдено, идет поиск в `globals()`. Если и там нет, то идет поиск в `__builtin__`. Стоит заметить, что `__builtin__` - модуль. При поиске объекта мы обращаемся к _его_ `locals()`.

Знание этой тонкости позволяет ускорить код:

```python
import math
from math import sin

def test1(x):
    res = 1
    for _ in range(1000):
        res += math.sin(x)
    return res


def test2(x):
    res = 1
    for _ in range(1000):
        res += sin(x)
    return res


def test3(x, sin=math.sin):
    res = 1
    for _ in range(1000):
        res += sin(x)
    return res
```

```ipython
In [1]: %timeit test1(123_456)
102 µs ± 7.15 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)

In [2]: %timeit test2(123_456)
93.5 µs ± 12.3 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)

In [3]: %timeit test3(123_456)
74.9 µs ± 4.99 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)
```

Ощутимую разницу во времени можно объяснить, посмотрев на байт-код:

```
-----------------------------
test1:

20 LOAD_GLOBAL      1 (math)
22 LOAD_METHOD      2 (sin)
24 LOAD_FAST        0 (x)
26 CALL_METHOD      1
-----------------------------
test2:

20 LOAD_GLOBAL      1 (sin) 
22 LOAD_FAST        0 (x)
24 CALL_FUNCTION    1
-----------------------------
test3:

20 LOAD_FAST        1 (sin)
22 LOAD_FAST        0 (x)
24 CALL_FUNCTION    1
-----------------------------
```

В первом случае сначала мы обращаемся к модулю `math`, а затем ищем в нем функцию `sin`, то есть происходит два обращения к пространству имен. 

Во втором случае обращение одно - сразу к функции `sin`, избегая поиск в пространстве `math`. 

В третьем случае при первом вызове идет обращение к импортированной функции `sin`, но ссылка на нее сохраняется в значениях аргументов по умолчанию у объекта функции - `test3.__defaults__`, поэтому при последующих вызовах `sin` будет вызываться быстрее.

Такие оптимизации имеют место, если функция вызывается миллионы раз. В ином случае лучше сделать код более читабельным.
